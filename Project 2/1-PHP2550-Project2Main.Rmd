---
title: "Baseline Characteristics as Potential Moderators and Predictors of Smoking Cessation in Adults with Major Depressive Disorder"
subtitle: "PHP2550 Project 2: A regression analysis"
author: "Yanwei (Iris) Tong"
date: "2024-10-10 (Revised 2024-12-10)"
output: 
  pdf_document:
    latex_engine: xelatex
    keep_tex: true
    #number_sections: true
link-citations: yes
header-includes:
  - "\\usepackage{xcolor}"
abstract: |
  **Purpose:** Building on a previous randomized, placebo-controlled study exploring factors influencing smoking cessation among adults with major depressive disorder (MDD), this project reexamined data from the same trial to accomplish the following two main objectives: 1) to identify baseline variables that may moderate the impact of behavioral treatments on end-of-treatment (EOT) smoking abstinence, and 2) to evaluate baseline variables as predictors of abstinence outcomes, accounting for the effects of both behavioral treatments and pharmacotherapy. 
  
  **Methods**: Missing data was imputed with MICE. Then, for each imputed data set, we applied logistic regression to model the binary smoking abstinence outcome and implemented three selection methods to identify moderating and predictive variables: lasso regression and elastic net regression with 10-fold cross-validation. Interaction terms were carefully incorporated, especially for factors hypothesized to moderate behavioral activation effects on abstinence. *Objective 1* included a comprehensive range of baseline variables and interaction terms with BA, while *Objective 2* focused solely on baseline predictors without interactions to assess their independent predictive effects. Calibration and discrimination metrics were used to evaluate model performance.     
  
  **Results and conclusion**: Our analysis identified several key baseline characteristics as potential moderators and predictors for smoking cessation among individuals with MDD. Interactions between BA and FTCD score, Nicotine Metabolism Ratio (NMR), readiness to quit, MDD status, and anhedonia suggested nuanced influences on cessation, potentially acting as moderators of treatment effects. For predictor effects, variables such as education level, race, FTCD score, and NMR had certain impacts on odds of abstinence. In terms of model performance, Elastic Net achieved the best calibration metrics, with the lowest Brier score and calibration error, whereas bidirectional stepwise and best subset exhibited slightly higher AUC values, reflecting marginally better discrimination. 
  

geometry: margin=1in
fontsize: 10.5pt
csl: 0-american-statistical-association.csl
bibliography: 0-references.bib
---



```{r setup, include=FALSE}
# to prevent scientific notation
options(scipen=999)

# Set up knit environment
knitr::opts_chunk$set(echo = FALSE, 
                      message = FALSE, 
                      warning = FALSE, 
                      error = FALSE)

# Load necessary packages
library(tidyverse)
library(kableExtra)
library(knitr)
library(ggplot2)
library(gridExtra)
library(grid)
library(naniar)
library(gtsummary)
library(gt)
library(patchwork)
library(knitcitations)
library(mice)
library(glmnet)
library(pROC)
library(MASS)
library(leaps)
library(RColorBrewer) 
library(cowplot)
library(caret)
```

```{r data-preprocess, results='hide'}
# Define data path and import data
data_path = "/Users/yanweitong/Documents/PHP2550-Data/Project2"
data = read.csv(paste0(data_path, "/project2.csv"))

mcar_test(data)

# Data preprocessing
data = data  %>%
  # create race variable
  mutate(race = factor(case_when(
    NHW == 1 ~ "Non-Hispanic white",
    Black == 1 ~ "Black",
    Hisp == 1 ~ "Hispanic",
    TRUE ~ "Other"  # Handle cases where none of the above conditions are met
  ), levels = c("Non-Hispanic white", "Black", "Hispanic", "Other"))) %>%
  # create treatment categories
  mutate(treatment_cat = factor(case_when(BA == 1 & Var == 0 ~ "BASC+placebo",
                                   BA == 0 & Var == 0 ~ "ST+placebo",
                                   BA == 1 & Var == 1 ~ "BASC+varenicline",
                                   BA == 0 & Var == 1 ~ "ST+varenicline"))) %>% 
  # factorize categorical/ordinal variables
  mutate(
    abst = factor(abst),
    Var = factor(Var),
    BA = factor(BA),
    sex_ps = factor(sex_ps),
    NHW = factor(NHW),
    Black = factor(Black),
    
    ftcd.5.mins = factor(ftcd.5.mins),
    otherdiag = factor(otherdiag),
    antidepmed = factor(antidepmed),
    mde_curr = factor(mde_curr),
    Only.Menthol = factor(Only.Menthol),
    edu = factor(edu, levels = c(1, 2, 3, 4, 5)),
    inc = factor(inc, levels = c(1, 2, 3, 4, 5))
  ) %>%
  # make integers numeric 
  mutate(across(
    .cols = where(is.integer) & !all_of("id"),
    .fns = as.numeric 
  ))
```

# \textcolor{orange}{INTRODUCTION}

This regression analysis project, in collaboration with Dr. George Papandonatos from Brown's Department of  Biostatistics, seeked to explore factors influencing smoking cessation among adults with major depressive disorder (MDD). Individuals with MDD often demonstrate stronger nicotine dependence and experience more challenging withdrawal symptoms than those without MDD. While varenicline is a proven aid for smoking cessation, addressing psychological factors associated with MDD-related smoking behaviors might also improve quit rates in this population.

Dr. Papandonatos' previous randomized, placebo-controlled study (@hitsman2023efficacy), which included 300 adult smokers with either current or past MDD, employed a 2x2 factorial design and compared behavioral activation for smoking cessation (BASC) against standard treatment (ST) and varenicline versus placebo. The multi-center study found no significant differences in abstinence outcomes between BASC and ST, regardless of varenicline use. However, varenicline significantly outperformed placebo at the 27-week follow-up, achieving a cessation of 16.2\% compared to 7.5\% for the placebo group. 


\newpage
\begin{landscape}
```{r table1}
# for sub-tab purpose
table1_data = data %>%
  mutate(
    Demographics = NA,
    Smoking = NA,
    Psychiatric = NA
  ) %>%
  mutate(edu = factor(edu, levels = c(1, 2, 3, 4, 5),
                 labels = c("Grade school", 
                            "Some high school", 
                            "High school graduate or GED", 
                            "Some college/technical school", 
                            "College graduate")),
    inc = factor(inc, levels = c(1, 2, 3, 4, 5),
                 labels = c("Less than $20,000", 
                            "$20,000–35,000", 
                            "$35,001–50,000", 
                            "$50,001–75,000", 
                            "More than $75,000")))
  
table1_data %>%
  dplyr::select(
    treatment_cat,
    Demographics,
    age_ps,
    sex_ps,
    race,
    inc,
    edu,
    Smoking,
    cpd_ps,
    ftcd_score,
    ftcd.5.mins,
    bdi_score_w00,
    crv_total_pq1,
    hedonsum_n_pq1,
    hedonsum_y_pq1,
    NMR,
    Only.Menthol,
    readiness,
    Psychiatric,
    shaps_score_pq1,
    otherdiag,
    antidepmed,
    mde_curr
  ) %>%
  tbl_summary(
    statistic = list(all_continuous() ~ c("{mean} ({sd})"),
                     all_categorical() ~ "{n} ({p}%)"),
    by = treatment_cat,
    digits = all_continuous() ~ 1,
    missing = "no",
    type = list(
      age_ps ~ "continuous",
      sex_ps ~ "dichotomous",
      race ~ "categorical",
      inc ~ "categorical",
      edu ~ "categorical",
      cpd_ps ~ "continuous", 
      ftcd_score ~ "continuous",
      ftcd.5.mins ~ "dichotomous",
      bdi_score_w00 ~ "continuous",
      crv_total_pq1 ~ "continuous",
      hedonsum_n_pq1 ~ "continuous",
      hedonsum_y_pq1 ~ "continuous",
      NMR ~ "continuous",
      Only.Menthol ~ "dichotomous",
      readiness ~ "continuous",
      shaps_score_pq1 ~ "continuous", 
      otherdiag ~ "dichotomous",
      antidepmed ~ "dichotomous",
      mde_curr ~ "dichotomous"
    ),
    label = list(
      age_ps = "Age (years)",
      sex_ps = "Sex (female)",
      race = "Race",
      inc = "Income",
      edu = "Education",
      cpd_ps = "Cigarettes per day",
      ftcd_score = "FTCD score",
      ftcd.5.mins = "Smoking with 5 mins of waking up (Yes)",
      bdi_score_w00 = "BDI score",
      crv_total_pq1 = "Cigarette reward value",
      hedonsum_n_pq1 = "Pleasurable Events Scale (substitute reinforcers)",
      hedonsum_y_pq1 = "Pleasurable Events Scale (complementary reinforcers)",
      Only.Menthol = "Exclusive mentholated cigarette user (Yes)",
      readiness = "Readiness to quit smoking",
      NMR = "Nicotine Metabolism Ratio",
      shaps_score_pq1 = "Anhedonia", 
      otherdiag = "Other lifetime DSM-5 diagnosis (Yes)",
      antidepmed = "Antidepressant medication (Yes)",
      mde_curr = "Current (and past) MDD vs past MDD only (Yes)"
    ),
    value = list(
      sex_ps ~ "2",
      Only.Menthol ~ "1",
      otherdiag ~ "1",
      antidepmed ~ "1",
      mde_curr ~ "1",
      ftcd.5.mins ~ "1"
    )
  ) %>%
  add_overall() %>%
  add_p() %>%
  modify_header(label ~ "**Characteristic**") %>%
  modify_caption(caption = "Participant characteristics by overall sample and treatment arm") %>%
  # for sub-tab purpose
  modify_table_body(
    ~ .x %>%
      mutate(across(everything(), ~ ifelse(. == "0 (NA%)", "", .)))
  ) %>%  
  modify_table_styling(
    rows = label %in% c("Sociodemographics", "Smoking", "Psychiatric"),
    columns = label,
    text_format = "bold"
  )   %>%
  as_kable_extra(
    booktabs = TRUE,
    longtable = TRUE,
    linesep = "",
    format = "latex"
  ) %>%
  kableExtra::kable_styling(
    position = "center",
    latex_options = c("striped", "repeat_header", "hold_position", "scale_down"),
    stripe_color = "gray!15",
    font_size = 8
  )
```

\end{landscape}
\newpage


Building on the original study, this project reexamined data from the same trial to accomplish the following two main objectives: 1) to identify baseline variables that may moderate the impact of behavioral treatments on end-of-treatment (EOT) smoking abstinence, and 2) to evaluate baseline variables as predictors of smaking cessation, accounting for the effects of both behavioral treatments and pharmacotherapy. 


# \textcolor{orange}{DATA}

## Data Overview 

The study population of the RCT consists of 300 adult smokers with a history of current or past MDD. 1) Sociodemographic, 2) smoking-related, and 3) psychiatric characteristics of the participants were collected at baseline and displayed in **Table 1** above. Demographics are age, sex, race, income, education; the smoking behaviors/measurements include key statistics like Fagerstrom Test for Cigarette Dependence (FTCD) score, Nicotine Metabolism Ratio (NMR), and indicator for exclusive Mentholated cigarette user; and psychiatric disgnotic and treatment history . The participants were randomized into four treatment arms: BASC with placebo (`BASC+placebo`), BASC with varenicline (`BASC+Varenicline`), ST with placebo (`ST+placebo`), and ST with varenicline (`ST+Varenicline`). 

Overall, the randomization process appears successful, as key variables such as demographic characteristics, smoking intensity (cigarettes per day), and psychiatric measures like the DSM-5 diagnosis and anhedonia scores showed similar distributions across the four treatment arms with *p*-value much greater than 5\%, suggesting that participant characteristics were well-balanced across treatment arms, as expected in an RCT. This balance across groups reinforces the original study's internal validity, as any differences in outcomes can be more confidently attributed to the interventions rather than baseline differences in participant characteristics.

Regarding the variable education, as shown in **Table 1**, two of the lowest education levels— grade school and some high school— had very few participant counts (1 and 16, respectively). To ensure adequate sample size and meaningful comparisons across education categories, we merged the first three levels (grade school, some high school, and high school graduate or GED) into a single category, considered as "High School and Below." This aggregation would improve the interpretability of the data by creating a more substantial subgroup and reducing variability, allowing for more reliable statistical analyses.

To explore the potential interaction effects between variables in the dataset, race versus exclusive menthol cigarette use would be a good example. As shown in the contingency table (**Table 2**), the distribution across racial groups is not balanced, as indicated by the significant Chi-square test statistic, suggesting that certain racial groups (Black in this case) might have a stronger inclination towards exclusive menthol use. This imbalance in distribution underscores the importance of considering racial demographics in our analysis, as they may interact with other smoking-related behaviors or biological factors.

```{r}
# Create a new variable that contains the 3 first levels of edu
data <- data %>%
  mutate(edu_merged = factor(case_when(
    edu %in% c("1", "2", "3") ~ "1",
    edu == "4" ~ "2",
    edu == "5" ~ "3"
  )))

```



```{r}
# Create and display the contingency table between race and menthol useage
table_race_menthol <- table(data$race, data$Only.Menthol)

# Perform the chi-square test
chi_square_test <- chisq.test(table_race_menthol)

chi_square_text <- paste0(
  sprintf("Chi-Square Statistic ≈ %.2f", chi_square_test$statistic),
  sprintf("， p-value ≈ %.4f", chi_square_test$p.value)
)

kable(table_race_menthol, 
      caption = "Contingency Table of Race vs. Only Menthol Use with Chi-Square Test Result", 
      col.names = c("Non-Menthol-Only", "Menthol-Only"),
      row.names = TRUE) %>%
   footnote(chi_square_text,
            footnote_as_chunk = FALSE) %>%
  kable_styling(full_width = F, position = "center", font_size = 9)

```

Similarly, the exploratory interaction heatmaps showcase four selected examples of interactions that would be explored further in our analysis. These examples illustrate potential interactions that may make biological or statistical sense in the context of smoking cessation. For instance, the interaction between behavioral activation and MDD diagnosis could be central to this study’s focus on using MDD-targeted treatments to aid smoking cessation. The interaction between income and education could reflect socioeconomic influences on smoking behaviors, while cigarette dependence (measured by FTCD score) and readiness to quit smoking might reveal motivational factors in cessation attempts. Additionally, nicotine metabolism ratio (NMR) versus sex could highlight a possible biological interaction that may affect the body’s response to nicotine between genders. The color gradients in **Figure 1** indicating proportion appear to confirm that there might be potential synergies between these variables. This visual evidence supported the inclusion of these interactions for further analysis, as they may capture meaningful relationships that influence smoking cessation. In the methods section, we would discuss a more comprehensive rationale for including a certain set of interaction terms, categorizing some as moderators specifically for BA effects and others as covariates, to capture these multidimensional influences on the smoking abstinence outcome.

```{r, fig.width = 10, fig.height =6, out.width="85%", fig.align='center'}
# Define function to bin continuous variables and create proportion heatmaps
create_heatmap <- function(data, var1, var2, title, bin_var1 = FALSE, 
                           bin_var2 = FALSE) {
  # Remove NA values for the specified columns
  data <- data %>% drop_na(all_of(c(var1, var2)))
  
  # Optionally bin continuous variables
  if (bin_var1) data <- data %>% 
      mutate(!!sym(var1) := cut(!!sym(var1), breaks = 4))  
  if (bin_var2) data <- data %>% 
      mutate(!!sym(var2) := cut(!!sym(var2), breaks = 4))  
  
  # Calculate proportions
  prop_data <- data %>%
    group_by(!!sym(var1), !!sym(var2)) %>%
    summarise(count = n(), .groups = 'drop') %>%
    mutate(prop = count / sum(count))
  
  ggplot(prop_data, aes_string(x = var1, y = var2)) +
    geom_tile(aes(fill = prop), color = "white") +
    scale_fill_gradientn(colors = brewer.pal(9, "Oranges"), name = "Proportion") +
    labs(title = title, x = var1, y = var2) +
    theme_minimal(base_size = 8) +  # Set smaller base font size
    theme(
      plot.title = element_text(size = 12, hjust = 0.5),
      axis.text.x = element_text(size = 9),
      axis.text.y = element_text(size = 9),
      axis.title.y = element_text(vjust = 0.5),
      legend.key.size = unit(0.45, "cm"),
      plot.margin = margin(1, 1, 1, 1)  # Add space around the plot
    )
}

# Create the plots with adjusted text and layout
p1 <- create_heatmap(data, "BA", "mde_curr", "BA vs. MDD Status")
p2 <- create_heatmap(data, "inc", "edu_merged", "Income vs. Education")
p3 <- create_heatmap(data, "NMR", "sex_ps", "NMR vs. Sex", bin_var1 = TRUE)  
p4 <- create_heatmap(data, "ftcd_score", "readiness", 
                     "FTCD Score vs. Readiness to Quit Smoking", 
                     bin_var1 = TRUE, bin_var2 = TRUE) 

# Arrange the plots in a grid with a title using cowplot
final_plot <- plot_grid(p1, p2, p3, p4, ncol = 2, align = "hv", 
                        rel_widths = c(1, 1), rel_heights = c(1, 1))

# Add a title to the entire grid
title <- ggdraw() + 
  draw_label("Figure 1: Exploratory Interaction Heatmaps-- Moderator and Covariate Examples", 
              size = 16)

# Combine title and grid plot
plot_grid(title, final_plot, ncol = 1, rel_heights = c(0.1, 1))
```



## Data Missingness and Imputation

```{r}
# Missingness table
# Calculate missing values for each variable
missing_summary <- data %>%
  summarise(across(everything(), ~ sum(is.na(.)), .names = "{col}")) %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Number") %>%
  mutate(Percent = (Number / nrow(data)) * 100) %>%
  filter(Number > 0)  # Exclude variables with 0 missingness

# Define a named vector with old and new names for variables
variable_names <- c(
  "inc" = "Income",
  "ftcd_score" = "FTCD score at baseline",
  "crv_total_pq1" = "Cigarette reward value at baseline",
  "shaps_score_pq1" = "Anhedonia",
  "NMR" = "Nicotine Metabolism Ratio",
  "Only.Menthol" = "Exclusive Mentholated Cigarette User",
  "readiness" = "Baseline readiness to quit smoking"
)

# Rename variables in the summary table
missing_summary <- missing_summary %>%
  mutate(Variable = recode(Variable, !!!variable_names))


# Calculate total missing values and total missing percentage
total_rows_with_missing <- sum(rowSums(is.na(data)) > 0)
total_rows_with_missing_pct <- (total_rows_with_missing / nrow(data)) * 100


missing_summary <- missing_summary %>%
  arrange(desc(Percent)) %>%
  mutate(Percent = sprintf("%.2f%%", Percent)) 

# Combine the total missingness row with the summary table
total_missing_row <- tibble(
  Variable = "Participants with any missingness",
  Number = total_rows_with_missing,
  Percent = sprintf("%.2f%%", total_rows_with_missing_pct)
)

missing_summary <- bind_rows(total_missing_row, missing_summary)

# Display the final table
missing_summary %>%
  kable(
    col.names = c("Variable", "Number", "Percent"), 
    caption = "Summary of Missing Values"
  ) %>%
  kable_styling(full_width = F, position = "center", font_size = 9)
```

We performed the Little's MCAR test on the data, and it gave a $p-$value of ~0.22, which rejects the null hypothesis that the missingness of the dataset is not completely at random. However, the overall missingness in the dataset is approximately 20\% (59/300), way exceeding the commonly accepted 5\% threshold for data completeness. This level of missing data could compromise the validity of the analysis if left unaddressed, as it may lead to biased results or loss of valuable information. To preserve the dataset's integrity and maintain statistical power, we opted to impute the missing values with the Multiple Imputation by Chained Equations (MICE) method. It allows for flexible handling of various data types and patterns of missingness, thereby maximizing the use of available information and improving the robustness of the analyses. Specifically, we set the number of imputations to $m=5$, used predictive mean matching, and iterated the imputation process for a maximum of 50 iterations to ensure reliable imputations.

```{r mice)}
# Perform MICE imputation
imputed_data <- mice(data, m = 5, method = "pmm", maxit = 50, seed = 2024,  printFlag = FALSE)
```



# \textcolor{orange}{METHODS}

## Variable Inclusion Criteria for Full Models

For *Objective 1*, psychotherapy (`BA`) is the primary predictor of interest, while pharmacotherapy (`Var`) is treated as a control variable. Thus, we ensured to include both of the treatments in all models as consistent factors. The major goal of *Objective 1* is to explore how various baseline characteristics might moderate the effect of behavioral treatment on smoking cessation. To achieve this, we included a range of interaction terms between `BA` and key sociodemographic, smoking-related, and psychiatric variables as potential moderators. These interaction terms allowed us to examine how different participant characteristics may influence the effectiveness of behavoriol activation in promoting abstinence. However, we excluded certain terms, such as interactions between `BA` and variables like indicator of smoking with 5 mins of waking up (`ftcd.5mins`) or cigarettes per day (`cpd_ps`), because these measures are components of the broader FTCD score (`ftcd_score`) and thus would provide redundant information. By prioritizing unique, non-overlapping terms, we aimed to create a comprehensive yet parsimonious model.

In addition to `BA` interaction terms as potential moderators, we included other interaction terms involving `Var` and various baseline characteristics as covariates. We believed these covariate interaction terms shall account for known associations that could impact treatment outcomes. For instance, as previously discussed, there was a recognized relationship between race and menthol-only cigarette use, which could affect smoking cessation success. Similarly, the synergy between factors like baseline readiness to quit smoking (`readiness`), Nicotine Metabolism Ratio (`NMR`), and MDD history (`mde_curr`) could have critical impacts on smoking behavior and mental health status and have plausible biological or statistical interactions with other sociodemographic and smoking-related variables. These interactions reflect meaningful covariate effects that contribute to a more nuanced understanding of how different factors influence treatment outcomes, enabling a more robust analysis of predictors and moderators in the context of smoking cessation for adults with MDD.

For *Objective 2*, the focus shifts to using baseline variables as predictors of smoking cessation outcomes, rather than as covariates or moderators. In this context, `BA` and `Var` are included as control variables across all models to account for the effects of behavioral and pharmacotherapy interventions. However, our goal here is to examine the predictive power of baseline characteristics independently, so we did not include any interaction terms to simplify the model by isolating the main effects of each baseline variable.

## Variable Selection Methods

The entire imputed dataset (N=300) was split into a training set (70\%) and a test set (30\%) to facilitate model evaluation and to ensure that the model's performance metrics would generalize beyond the training data. This approach allowed us to assess the predictive accuracy and robustness of the selected models on an independent dataset, which is critical for reducing overfitting and improving the reliability of our findings.

### 1) Lasso Regression

Lasso regression, or Least Absolute Shrinkage and Selection Operator, is a regularization technique that applies an L1 penalty to the regression coefficients. This penalty has the unique property of shrinking some coefficients exactly to zero, which effectively performs variable selection. Lasso is particularly advantageous in situations with high-dimensional data where there are many predictors but only a subset is truly relevant to the outcome. By setting irrelevant coefficients to zero, Lasso simplifies the model, improves interpretability, and reduces overfitting.

In our analysis, Lasso was used to identify a parsimonious set of predictors that best explained the outcome. Cross-validation was employed to select the optimal lambda, ensuring that the model achieved a balance between sparsity (variable selection) and predictive accuracy. While Lasso works well for sparse datasets, it can struggle when predictors are highly correlated, as it tends to arbitrarily select one variable from a group of correlated variables, which may not always lead to the most stable solution.

### 2) Elastic Net Regression
Elastic Net regression is a regularization and variable selection technique that incorporates both Lasso (L1 penalty) and Ridge (L2 penalty) regression methods. It addresses some limitations of Lasso, particularly in situations where predictors are highly correlated. By combining L1 and L2 penalties, Elastic Net encourages a grouping effect where correlated variables tend to be selected or excluded together, leading to more stable and reliable models.

In our modeling, Elastic Net was utilized to take advantage of both variable shrinkage and selection properties of Lasso and the grouping effect of Ridge regression. This method helps handle multi-collinearity among predictors while performing variable selection. We used cross-validation to determine the optimal value of the regularization parameter lambda, specifically selecting the lambda that minimizes the cross-validation error ($\lambda_{min}$). This approach ensures that the model achieves the best predictive performance on unseen data by effectively balancing bias and variance.


## Performance Metrics (Calibration and Discrimination)

To evaluate model performances, we used a combination of calibration and discrimination measures. Calibration plots with error bars and LOESS smoothing allowed us to visually assess the agreement between predicted probabilities and observed outcomes, indicating how well-calibrated the model is across different probability levels. The addition of error bars provides insights into the variability of predictions, while LOESS smoothing offers a flexible fit to better capture trends in calibration. The ROC curve evaluates the model's discrimination ability, reflecting its capability to distinguish between positive and negative outcomes. Furthermore, quantitative metrics such as Brier score and calibration error were included in tables (**Table 5** and **Table 7**) to provide objective assessments of model accuracy and calibration. Brier score combines both calibration and sharpness of probability estimates, while calibration error specifically measures the deviation between predicted and observed probabilities, providing complementary insights into model performance beyond visual assessments.

# \textcolor{orange}{RESULTS}

## *Objective 1*: Baseline Characteristics as Potential Moderators 

To facilitate interpretation, we exponentiate the coefficients ($exp(\beta)$) to express them as odds ratios, which indicate the multiplicative change in the odds of the outcome for each one-unit increase in a predictor, holding other variables constant. In **Table 4**, we observed the coefficients and corresponding odds ratios selected from three model selection methods, with particular focus on behavioral activation and its interaction terms. `BA` was consistently included across all methods, as was pharmacotherapy (`Var`), given their forced inclusion as primary predictor and control. Among the interaction terms with `BA` (`BA1` = BASC) several variables demonstrated potential moderator effects that might influence BA's effectiveness in smoking cessation among individuals with MDD, although there is noticeable variation in terms selected by each method. 

Specifically, BASC's interaction with FTCD score with odds ratio (OR) $< 1$ may suggest that nicotine dependence level could negatively affect how responsive individuals are to BASC. Similarly, current vs. past MDD status (`mde_curr`) appeared as a logical moderator, as BA was designed to alleviate depressive symptoms, which could make it more effective for individuals experiencing active symptoms. The interaction with readiness to quit smoking (`readiness`) with OR slightly higher than 1 in best subset selection may suggest that motivational factors might influence how beneficial BASC is; those who are more prepared to quit may respond differently to BASC. Anhedonia as a potential moderator also aligns well, given that BASC aims to counteract reduced pleasure in activities, a common symptom in depression. Finally, race (Hispanic) as an interaction term may reflect unique socio-cultural factors that could impact the treatment response to BASC. These interaction terms, chosen by at least one method, underscored relevant moderators that may affect psychotherapy's success in achieving smoking cessation.

Noticeably, the inclusion of Anhedonia (`shaps_score_pq1)` and Race Black (`raceBlack`), as non-interaction variables, across three methods may suggest these two factors could play a meaningful role in the overall relationship between BA and smoking cessation. For example, individuals with higher anhedonia levels may require more intensive or targeted interventions to achieve abstinence, highlighting the importance of tailoring BA to address depressive symptoms. Similarly, the repeated selection of race Black as a covariate could imply that racial background could affect treatment outcomes, potentially due to sociocultural or economic factors. 

For the majority of other interaction terms, selection varied by method, with little overlap between the chosen variables, reflecting different approaches and model assumptions inherent in bidirectional stepwise, Elastic Net, and best subset selection. This variation potentially emphasizes the differences in variable selection procedure and criteria across the three methods.


```{r}
# To identify the potential interaction terms for moderator effects
train_variables_dummy_include_names <- c(
  "Var1", "BA1", "age_ps", "sex_ps2", "inc2", "inc3", 
  "inc4", "inc5", "edu_merged2", "edu_merged3",
  "raceBlack", "raceHispanic", "raceOther", 
  "ftcd_score", "ftcd.5.mins1", "bdi_score_w00", "cpd_ps",
  "crv_total_pq1", "hedonsum_n_pq1", "hedonsum_y_pq1",              
  "shaps_score_pq1", "otherdiag1", "antidepmed1",                  
  "mde_curr1", "NMR", "Only.Menthol1",                 
  "readiness", "Var1:BA1", 
  #Behavioral treatment (MAIN)
  "BA1:mde_curr1", 
  "BA1:age_ps", "BA1:sex_ps2", 
  "BA1:raceBlack", "BA1:raceHispanic",
  "BA1:raceOther", "BA1:ftcd_score", 
  "BA1:shaps_score_pq1","BA1:bdi_score_w00", 
  "BA1:otherdiag1", "BA1:antidepmed1",
  "BA1:mde_curr1","BA1:NMR",
  "BA1:Only.Menthol1", "BA1:readiness",
  # Pharmacotherapy
  "Var1:mde_curr1",
  "Var1:age_ps", "Var1:sex_ps2",
  "Var1:raceBlack", "Var1:raceHispanic",
  "Var1:raceOther", "Var1:ftcd_score", 
  # Income*Edu
  "inc2:edu_merged2", "inc2:edu_merged3", 
  "inc3:edu_merged2", "inc3:edu_merged3", 
  "inc4:edu_merged2", "inc4:edu_merged3", 
  "inc5:edu_merged2", "inc5:edu_merged3", 
  # Readiness to quit
  "Only.Menthol1:readiness", 
  "mde_curr1:readiness", "ftcd_score:readiness", 
  # FTCD Score 
  "sex_ps2:ftcd_score", "raceBlack:ftcd_score", 
  "raceHispanic:ftcd_score", "raceOther:ftcd_score", 
  "age_ps:ftcd_score", 
  # Menthol exclusive
  "sex_ps2:Only.Menthol1", "raceBlack:Only.Menthol1", 
  "raceHispanic:Only.Menthol1", "raceOther:Only.Menthol1", 
  "inc2:Only.Menthol1", "inc3:Only.Menthol1", 
  "inc4:Only.Menthol1", "inc5:Only.Menthol1", 
  "edu_merged2:Only.Menthol1", "edu_merged3:Only.Menthol1", 
  # NMR
  "sex_ps2:NMR", "age_ps:NMR", "cpd_ps:NMR", 
  "NMR:readiness", "ftcd_score:NMR"
)


```

```{r helper-functions}
variable_names <- c("Var", "BA", "age_ps", "sex_ps", "inc", "edu_merged", "race",
                     "ftcd_score", "ftcd.5.mins", "bdi_score_w00", "cpd_ps",
                     "crv_total_pq1", "hedonsum_n_pq1", "hedonsum_y_pq1",
                     "shaps_score_pq1", "otherdiag", "antidepmed", "mde_curr",
                     "NMR", "Only.Menthol", "readiness")

# Helper function to perform model fitting
fit_models <- function(data) {
  # Define predictors and outcome
  
  outcome <- data$abst
  variables <- data[, variable_names]
  # for Lasso (to break down factors with >2 levels)
  variables_dummy <- model.matrix( ~ 0 + ., data = variables)
  # remove the extra reference group
  variables_dummy <- variables_dummy[, -which(colnames(variables_dummy) ==
                                                "Var0")]
  

  # Split into train and test sets
  set.seed(46)
  train_index <- createDataPartition(outcome, p = 0.7, list = FALSE)
  train_data <- data[train_index, ]
  train_outcome = train_data$abst
  test_data <- data[-train_index, ]
  test_outcome = test_data$abst
  
  train_variables_dummy <- variables_dummy[train_index, ]
  test_variables_dummy <- variables_dummy[-train_index, ]
  
  # ^2 generates all pairwise interactions
  train_variables_dummy_df <- as.data.frame(train_variables_dummy)
  train_variables_dummy_full_interactions <- model.matrix( ~ . ^ 2, data = train_variables_dummy_df)
  
  test_variables_dummy_df <- as.data.frame(test_variables_dummy)
  test_variables_dummy_full_interactions <- model.matrix( ~ . ^ 2, data = test_variables_dummy_df)
  
  train_variables_dummy_include =
    train_variables_dummy_full_interactions[, train_variables_dummy_include_names]
  test_variables_dummy_include =
    test_variables_dummy_full_interactions[, train_variables_dummy_include_names]
  
  # Set penalty factors to enforce keeping Var and BA
  # Initialize penalty factors to 1 for all variables
  penalty_factors <- rep(1, ncol(train_variables_dummy_include))

  # Identify columns corresponding exactly to "Var1" and "BA1" (not their interactions)
  var1_col <- grep("^Var1$", colnames(train_variables_dummy_include))
  ba1_col <- grep("^BA1$", colnames(train_variables_dummy_include))
  
  penalty_factors[c(var1_col, ba1_col)] <- 0
  names(penalty_factors) <- colnames(train_variables_dummy_include)

  # Fit Elastic Net model
  enet_model <- cv.glmnet(
    as.matrix(train_variables_dummy_include),
    train_outcome,
    penalty.factor = penalty_factors,
    alpha = 0.5,
    family = "binomial",
    nfold = 10
  )

  # Fit Lasso model (alpha = 1)
  lasso_model <- cv.glmnet(
    as.matrix(train_variables_dummy_include),
    train_outcome,
    penalty.factor = penalty_factors,
    alpha = 1,
    family = "binomial",
    nfold = 10
  )

  list(
    elastic_net = enet_model,
    lasso = lasso_model
  )
}
```

```{r fit-multiple-imputed}
# Fit models across imputed datasets
aim1_results <- list()
for (i in 1:5) {
  dataset <- complete(imputed_data, action = i)
  aim1_results[[i]] <- fit_models(dataset)
}
```



```{r}
# Initialize lists to store coefficients for Elastic Net and Lasso
elastic_net_coefs <- lapply(aim1_results, function(res) coef(res$elastic_net, s = res$elastic_net$lambda.min))
lasso_coefs <- lapply(aim1_results, function(res) coef(res$lasso, s = res$lasso$lambda.min))

# Function to process coefficients for averaging and selection count
process_coefs <- function(coefs_list) {
  # Combine coefficients into a matrix
  coefs_matrix <- do.call(cbind, lapply(coefs_list, function(coef) as.numeric(coef)))
  rownames(coefs_matrix) <- rownames(coefs_list[[1]])
  
  # Count non-zero selections for each variable
  selection_counts <- rowSums(coefs_matrix != 0)
  
  # Compute the average of coefficients only when selected (non-zero)
  averaged_coefs <- rowSums(coefs_matrix) / selection_counts
  averaged_coefs[is.na(averaged_coefs)] <- 0  # Handle cases where selection_counts is 0
  
  # Create result table
  result_table <- data.frame(
    variable = rownames(coefs_matrix),
        SelectionTimes = selection_counts,
    AverageCoefficient = averaged_coefs
  ) %>%
    filter(SelectionTimes > 0)  # Keep only variables selected at least once
  
  return(result_table)
}

# Process Elastic Net and Lasso coefficients
result_table_enet <- process_coefs(elastic_net_coefs)
result_table_lasso <- process_coefs(lasso_coefs)

# Display the results
# result_table_enet
# result_table_lasso

```


```{r}
# Summary table of coef
large_threshold <- 100

# Calculate OR for each coefficient for each method and rename columns correctly
enet_df <- result_table_enet %>%
  rename(`Selection Times` = SelectionTimes) %>%
  rename(`Elastic Net Average Coef` = AverageCoefficient) %>%
  mutate(`Elastic Net Average OR` = exp(`Elastic Net Average Coef`))

lasso_df <- result_table_lasso %>%
  rename(`Selection Times` = SelectionTimes) %>%
  rename(`Lasso Average Coef` = AverageCoefficient) %>%
  mutate(`Lasso OR` = exp(`Lasso Average Coef`))



# Merge all data frames based on variable names
combined_df <- full_join(lasso_df, enet_df, by = "variable") %>%
  mutate(across(where(is.numeric), ~ round(.x, 4))) %>%
  mutate(across(where(is.numeric), ~ ifelse(as.numeric(.) > large_threshold, "*", .)))

# Remove the intercept row
combined_df <- combined_df[combined_df$variable != "(Intercept)", ]

# Standardize interaction terms by sorting them alphabetically
combined_df <- combined_df %>%
  mutate(
    variable = sapply(variable, function(x) {
      terms <- unlist(strsplit(x, ":"))
      if (length(terms) > 1) {
        paste(sort(terms), collapse = ":")
      } else {
        x
      }
    })
  )

# Combine rows with the same standardized interaction term names
combined_df <- combined_df %>%
  group_by(variable) %>%
  summarize(across(everything(), ~ ifelse(is.numeric(.), sum(as.numeric(.), na.rm = TRUE), .))) %>%
  ungroup()

# Replace zeroes and any remaining NAs with an empty space for readability
combined_df <- combined_df %>%
  mutate(across(where(is.numeric), ~ ifelse(. == 0, "", .))) %>%
  replace(is.na(.), " ")

# Display the final combined table with grouped headers
combined_df %>%
  kable(row.names = F, 
        col.names = c("Variable", "Selection Times","Average Coef", "OR", "Selection Times", "Average Coef", "OR"),
        caption = "Summary of Average Coefficients and Odds Ratios for Potential Moderator Effects across Model Selection Methods") %>%
  add_header_above(c(" " = 1, "Lasso" = 3, "Elastic Net" = 3)) %>%
  kable_styling(full_width = F, position = "center", font_size = 9)

```


```{r}
# Initialize lists to store results for each imputation
roc_results_enet <- list()
calib_results_enet <- list()
auc_values_enet <- list()

roc_results_lasso <- list()
calib_results_lasso <- list()
auc_values_lasso <- list()

num_cuts <- 10  # Number of bins for calibration

for (i in 1:5) {
  dataset <- complete(imputed_data, action = i)
  
  # Split the dataset into training and test sets
  set.seed(46)
  train_index <- createDataPartition(dataset$abst, p = 0.7, list = FALSE)
  train_data <- dataset[train_index, ]
  test_data <- dataset[-train_index, ]
  
  # Prepare predictors and outcome for test set
  test_variables_dummy <- model.matrix(~ 0 + ., data = test_data[, variable_names])
  test_variables_dummy <- test_variables_dummy[, -which(colnames(test_variables_dummy) == "Var0")]
  test_variables_dummy_full_interactions <- model.matrix(~ . ^ 2, data = as.data.frame(test_variables_dummy))
  test_variables_dummy_include <- test_variables_dummy_full_interactions[, train_variables_dummy_include_names]
  test_outcome <- test_data$abst

  # Predict probabilities for Elastic Net
  predicted_prob_enet <- as.numeric(predict(aim1_results[[i]]$elastic_net,
                                            newx = as.matrix(test_variables_dummy_include),
                                            s = "lambda.min", type = "response"))
  
  # Predict probabilities for Lasso
  predicted_prob_lasso <- as.numeric(predict(aim1_results[[i]]$lasso,
                                             newx = as.matrix(test_variables_dummy_include),
                                             s = "lambda.min", type = "response"))

  # Compute ROC and AUC for Elastic Net
  roc_enet <- roc(test_outcome, predicted_prob_enet)
  roc_results_enet[[i]] <- data.frame(
    Specificity = rev(roc_enet$specificities),
    Sensitivity = rev(roc_enet$sensitivities)
  )
  auc_values_enet[[i]] <- auc(roc_enet)  # Store AUC separately
  
  # Compute ROC and AUC for Lasso
  roc_lasso <- roc(test_outcome, predicted_prob_lasso)
  roc_results_lasso[[i]] <- data.frame(
    Specificity = rev(roc_lasso$specificities),
    Sensitivity = rev(roc_lasso$sensitivities)
  )
  auc_values_lasso[[i]] <- auc(roc_lasso)  # Store AUC separately

  
  # Calibration for Elastic Net
  calib_data_enet <- data.frame(
    prob = predicted_prob_enet,
    bin = cut(predicted_prob_enet, breaks = num_cuts),
    class = as.numeric(test_outcome) - 1
  )
  calib_results_enet[[i]] <- calib_data_enet %>%
    group_by(bin) %>%
    summarise(
      observed = mean(class),
      predicted = mean(prob),
      se = sqrt(observed * (1 - observed) / n())
    )
  
  # Calibration for Lasso
  calib_data_lasso <- data.frame(
    prob = predicted_prob_lasso,
    bin = cut(predicted_prob_lasso, breaks = num_cuts),
    class = as.numeric(test_outcome) - 1
  )
  calib_results_lasso[[i]] <- calib_data_lasso %>%
    group_by(bin) %>%
    summarise(
      observed = mean(class),
      predicted = mean(prob),
      se = sqrt(observed * (1 - observed) / n())
    )
}


```

```{r}
# Extract numeric AUC values from the list of AUC objects
auc_numeric_enet <- sapply(auc_values_enet, function(x) as.numeric(x))
auc_numeric_lasso <- sapply(auc_values_lasso, function(x) as.numeric(x))

# Compute the mean AUC
mean_auc_enet <- mean(auc_numeric_enet)
mean_auc_lasso <- mean(auc_numeric_lasso)

```


```{r}
# Define a common set of specificity thresholds
common_specificities <- seq(0, 1, length.out = 100)

# Interpolate sensitivity for each ROC curve at the common specificities
interp_sensitivities_lasso <- sapply(roc_results_lasso, function(roc_data) {
  approx(x = roc_data$Specificity, y = roc_data$Sensitivity, xout = common_specificities)$y
})

# Compute the mean sensitivity across imputations
mean_sensitivity_lasso <- rowMeans(interp_sensitivities_lasso, na.rm = TRUE)

# Create a data frame for the averaged ROC curve
mean_roc_lasso <- data.frame(
  Specificity = common_specificities,
  Sensitivity = mean_sensitivity_lasso
)

# Plot the averaged ROC curve
ROC_lasso = ggplot(mean_roc_lasso, aes(x = 1 - Specificity, y = Sensitivity)) +
  geom_line(color = "black", size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "grey") +
  annotate("text", x = 0.8, y = 0.2, label = paste("Mean AUC =", round(mean_auc_lasso, 2)), size = 3, color = "Black") +
  labs(
    x = "1 - Specificity",
    y = "Sensitivity"
  ) +
  theme_minimal()

```

```{r}
# Define a common set of specificity thresholds
common_specificities <- seq(0, 1, length.out = 100)

# Interpolate sensitivity for each ROC curve at the common specificities
interp_sensitivities_enet <- sapply(roc_results_enet, function(roc_data) {
  approx(x = roc_data$Specificity, y = roc_data$Sensitivity, xout = common_specificities)$y
})

# Compute the mean sensitivity across imputations
mean_sensitivity_enet <- rowMeans(interp_sensitivities_enet, na.rm = TRUE)

# Create a data frame for the averaged ROC curve
mean_roc_enet <- data.frame(
  Specificity = common_specificities,
  Sensitivity = mean_sensitivity_enet
)

# Plot the averaged ROC curve
ROC_enet = ggplot(mean_roc_enet, aes(x = 1 - Specificity, y = Sensitivity)) +
  geom_line(color = "black", size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "grey") +
  annotate("text", x = 0.8, y = 0.2, label = paste("Mean AUC =", round(mean_auc_enet, 2)), size = 3, color = "Black") +
  labs(
    x = "1 - Specificity",
    y = "Sensitivity"
  ) +
  theme_minimal()

```


```{r}
# Combine Calibration Results Across Imputations
calib_combined_enet <- do.call(rbind, calib_results_enet) %>%
  group_by(bin) %>%
  summarise(
    observed = mean(observed),
    predicted = mean(predicted),
    se = sqrt(sum(se^2) / n())
  )

calib_combined_lasso <- do.call(rbind, calib_results_lasso) %>%
  group_by(bin) %>%
  summarise(
    observed = mean(observed),
    predicted = mean(predicted),
    se = sqrt(sum(se^2) / n())
  )

```

```{r}
num_cuts <- 10  # Number of bins for calibration

# Add Loess Fit for Flexible Calibration Line
loess_fit <- loess(observed ~ predicted, data = calib_combined_lasso, span = 0.75)
calib_combined_lasso$loess_pred <- predict(loess_fit, calib_combined_lasso$predicted)

# Plot Calibration Curve with Error Bars
calib_error_bar_lasso = ggplot(calib_combined_lasso) + 
  geom_abline(intercept = 0, slope = 1, color = "red") + 
  geom_errorbar(aes(x = predicted, ymin = observed - 1.96 * se, 
                    ymax = observed + 1.96 * se), 
                colour="black", width=.01)+
  geom_point(aes(x = predicted, y = observed)) +
  labs(x = "Expected Probability of Smoking Abstinence", 
       y = "Actual Smoking Abstinence") +
  #       title = "Calibration Plot for Elastic Net Model with Error Bars"
  theme_minimal()


# Plot Calibration Curve with Loess
calib_combined_lasso <- calib_combined_lasso %>%
  mutate(loess_ci_lower = loess_pred - 1.96 * sd(loess_pred),
         loess_ci_upper = loess_pred + 1.96 * sd(loess_pred))

calib_loess_lasso = ggplot(calib_combined_lasso, aes(x = predicted, y = observed)) +
  # Flexible calibration (Loess)
  geom_line(aes(y = loess_pred), color = "blue", linetype = "dashed") +  
  geom_ribbon(aes(ymin = loess_ci_lower, ymax = loess_ci_upper), alpha = 0.2, fill = "grey") +
  geom_abline(intercept = 0, slope = 1, color = "red") +  # Perfect calibration line
  scale_color_manual(values = c("Ideal" = "red", 
                                "Flexible calibration" = "blue")) +
  scale_linetype_manual(values = c("Ideal" = "solid", 
                                   "Flexible calibration" = "dashed")) +
  labs(x = "Predicted Probability of Smoking Abstinence", 
       y = "Actual Smoking Abstinence",
       color = "Legend", linetype = "Legend") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) 

```


```{r}
num_cuts <- 10  # Number of bins for calibration

# Add Loess Fit for Flexible Calibration Line
loess_fit <- loess(observed ~ predicted, data = calib_combined_enet, span = 0.75)
calib_combined_enet$loess_pred <- predict(loess_fit, calib_combined_enet$predicted)

# Plot Calibration Curve with Error Bars
calib_error_bar_enet = ggplot(calib_combined_enet) + 
  geom_abline(intercept = 0, slope = 1, color = "red") + 
  geom_errorbar(aes(x = predicted, ymin = observed - 1.96 * se, 
                    ymax = observed + 1.96 * se), 
                colour="black", width=.01)+
  geom_point(aes(x = predicted, y = observed)) +
  labs(x = "Expected Probability of Smoking Abstinence", 
       y = "Actual Smoking Abstinence") +
  #       title = "Calibration Plot for Elastic Net Model with Error Bars"
  theme_minimal()


# Plot Calibration Curve with Loess
calib_combined_enet <- calib_combined_enet %>%
  mutate(loess_ci_lower = loess_pred - 1.96 * sd(loess_pred),
         loess_ci_upper = loess_pred + 1.96 * sd(loess_pred))

calib_loess_enet = ggplot(calib_combined_enet, aes(x = predicted, y = observed)) +
  # Flexible calibration (Loess)
  geom_line(aes(y = loess_pred), color = "blue", linetype = "dashed") +  
  geom_ribbon(aes(ymin = loess_ci_lower, ymax = loess_ci_upper), alpha = 0.2, fill = "grey") +
  geom_abline(intercept = 0, slope = 1, color = "red") +  # Perfect calibration line
  scale_color_manual(values = c("Ideal" = "red", 
                                "Flexible calibration" = "blue")) +
  scale_linetype_manual(values = c("Ideal" = "solid", 
                                   "Flexible calibration" = "dashed")) +
  labs(x = "Predicted Probability of Smoking Abstinence", 
       y = "Actual Smoking Abstinence",
       color = "Legend", linetype = "Legend") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) 

```


```{r, fig.width = 10, fig.height = 6, out.width="90%", fig.align='center'}

plots_enet = arrangeGrob(
  calib_error_bar_enet, calib_loess_enet,  ROC_enet,
  ncol = 3,
  top = textGrob("Elastic Net Regression",
                 gp = gpar(fontface = "bold", fontsize = 14)
))

plots_lasso = arrangeGrob(
  calib_error_bar_lasso, calib_loess_lasso, ROC_lasso,
  ncol = 3,
  top = textGrob("Lasso Regression", 
                 gp = gpar(fontface = "bold", fontsize = 14)
))

# Bold the main title
main_title <- textGrob(
  "Figure 2: Calibration Plots with Error Bars and LOESS and ROC Curves (Moderator Effects)",
  gp = gpar(fontsize = 16) 
)

# Arrange everything with the bold title
grid.arrange(
  plots_lasso,
  plots_enet,
  nrow = 2,
  top = main_title
)
```

The performance metrics presented in **Table 5** demonstrate the relative strengths and weaknesses of each model selection method for capturing moderator effects on smoking abstinence. Elastic Net exhibited the lowest Brier score (0.1422) and calibration error (0.0677), indicating superior model calibration and a reduced prediction error compared to stepwise and best subset selection. Moreover, Elastic Net achieved a balanced specificity (0.6351) and sensitivity (0.6875), suggesting a more robust discrimination capability across different abstinence probabilities. In contrast, the bidirectional stepwise model demonstrated a higher sensitivity (0.7500) but at the cost of reduced specificity (0.5270), implying a tendency toward over-predicting abstinence outcomes. The best subset model achieved the highest sensitivity (0.9375) but with markedly low specificity (0.3108), indicating that it may overfit to predict abstinence with lower precision across abstinence probabilities. The differences in calibration and discrimination metrics highlight Elastic Net’s ability to provide a well-calibrated model with balanced specificity and sensitivity, which may make it a preferable choice for models that aim to generalize well to unseen data.




The calibration and ROC curves in **Figure 2** further illustrate these performance characteristics. The calibration plots for Elastic Net closely align with the ideal diagonal line, suggesting that its predicted abstinence probabilities align well with observed outcomes across different probability levels. Additionally, the LOESS curve for Elastic Net generally tracks the ideal line with only minor deviations, reinforcing its effective calibration. In comparison, bidirectional stepwise and best subset exhibit some misalignment in their calibration plots, with deviations that suggest over- or under-confidence in specific probability ranges. The ROC curves also support Elastic Net's performance, as it demonstrates a moderate AUC of 0.6765, slightly surpassing stepwise and best subset, which achieved AUCs of 0.6503 and 0.6174, respectively. Elastic Net's ROC curve reflects a stronger balance between true positive and false positive rates, contributing to its more consistent predictive performance in modeling moderator effects.



## *Objective 2*: Baseline Characteristics as Potential Predictors


```{r}
train_predictors_dummy_include_names <- c(
  "Var1", "BA1", "age_ps", "sex_ps2", "inc2", "inc3",
  "inc4", "inc5", "edu_merged2", "edu_merged3",
  "raceBlack", "raceHispanic", "raceOther",
  "ftcd_score", "ftcd.5.mins1", "bdi_score_w00", "cpd_ps",
  "crv_total_pq1", "hedonsum_n_pq1", "hedonsum_y_pq1",
  "shaps_score_pq1", "otherdiag1", "antidepmed1",
  "mde_curr1", "NMR", "Only.Menthol1",
  "readiness"
)

 predictor_names <- c("Var", "BA", "age_ps", "sex_ps", "inc", "edu_merged", "race",
                     "ftcd_score", "ftcd.5.mins", "bdi_score_w00", "cpd_ps",
                     "crv_total_pq1", "hedonsum_n_pq1", "hedonsum_y_pq1",
                     "shaps_score_pq1", "otherdiag", "antidepmed", "mde_curr",
                     "NMR", "Only.Menthol", "readiness")

# Helper function to perform model fitting
 fit_models <- function(data) {
   # Define predictors and outcome
   outcome <- data$abst
   
   predictors <- data[, predictor_names]
   # for Lasso (to break down factors with >2 levels)
   predictors_dummy <- model.matrix( ~ 0 + ., data = predictors)
   # remove the extra reference group
   predictors_dummy <- predictors_dummy[, -which(colnames(predictors_dummy) ==
                                                   "Var0")]
   
  

  # Split into train and test sets
   set.seed(46)
   train_index <- createDataPartition(outcome, p = 0.7, list = FALSE)
   train_data <- data[train_index, ]
   train_outcome = train_data$abst
   test_data <- data[-train_index, ]
   test_outcome = test_data$abst
   
   train_predictors_dummy <- predictors_dummy[train_index, ]
   test_predictors_dummy <- predictors_dummy[-train_index, ]
   
   train_predictors_dummy_include =
     train_predictors_dummy[, train_predictors_dummy_include_names]
   test_predictors_dummy_include =
     test_predictors_dummy[, train_predictors_dummy_include_names]

  # Set penalty factors to enforce keeping Var and BA
  # Initialize penalty factors to 1 for all variables
  penalty_factors <- rep(1, ncol(train_predictors_dummy_include))

  # Identify columns corresponding exactly to "Var1" and "BA1" (not their interactions)
  var1_col <- grep("^Var1$", colnames(train_predictors_dummy_include))
  ba1_col <- grep("^BA1$", colnames(train_predictors_dummy_include))
  
  penalty_factors[c(var1_col, ba1_col)] <- 0
  names(penalty_factors) <- colnames(train_predictors_dummy_include)

  # Fit Elastic Net model
  enet_model <- cv.glmnet(
    as.matrix(train_predictors_dummy_include),
    train_outcome,
    penalty.factor = penalty_factors,
    alpha = 0.5,
    family = "binomial",
    nfold = 10
  )

  # Fit Lasso model (alpha = 1)
  lasso_model <- cv.glmnet(
    as.matrix(train_predictors_dummy_include),
    train_outcome,
    penalty.factor = penalty_factors,
    alpha = 1,
    family = "binomial",
    nfold = 10
  )

  list(
    elastic_net = enet_model,
    lasso = lasso_model
  )
}
```

```{r}
# Fit models across imputed datasets
aim2_results <- list()
for (i in 1:5) {
  dataset <- complete(imputed_data, action = i)
  aim2_results[[i]] <- fit_models(dataset)
}
```



```{r}
# Initialize lists to store coefficients for Elastic Net and Lasso
elastic_net_coefs <- lapply(aim2_results, function(res) coef(res$elastic_net, s = res$elastic_net$lambda.min))
lasso_coefs <- lapply(aim2_results, function(res) coef(res$lasso, s = res$lasso$lambda.min))

# Function to process coefficients for averaging and selection count
process_coefs <- function(coefs_list) {
  # Combine coefficients into a matrix
  coefs_matrix <- do.call(cbind, lapply(coefs_list, function(coef) as.numeric(coef)))
  rownames(coefs_matrix) <- rownames(coefs_list[[1]])
  
  # Count non-zero selections for each variable
  selection_counts <- rowSums(coefs_matrix != 0)
  
  # Compute the average of coefficients only when selected (non-zero)
  averaged_coefs <- rowSums(coefs_matrix) / selection_counts
  averaged_coefs[is.na(averaged_coefs)] <- 0  # Handle cases where selection_counts is 0
  
  # Create result table
  result_table <- data.frame(
    variable = rownames(coefs_matrix),
        SelectionTimes = selection_counts,
    AverageCoefficient = averaged_coefs
  ) %>%
    filter(SelectionTimes > 0)  # Keep only variables selected at least once
  
  return(result_table)
}

# Process Elastic Net and Lasso coefficients
result_table_enet <- process_coefs(elastic_net_coefs)
result_table_lasso <- process_coefs(lasso_coefs)

# Display the results
# result_table_enet
# result_table_lasso

```


```{r}
# Summary table of coef
large_threshold <- 100

# Calculate OR for each coefficient for each method and rename columns correctly
enet_df <- result_table_enet %>%
  rename(`Selection Times` = SelectionTimes) %>%
  rename(`Elastic Net Average Coef` = AverageCoefficient) %>%
  mutate(`Elastic Net Average OR` = exp(`Elastic Net Average Coef`))

lasso_df <- result_table_lasso %>%
  rename(`Selection Times` = SelectionTimes) %>%
  rename(`Lasso Average Coef` = AverageCoefficient) %>%
  mutate(`Lasso OR` = exp(`Lasso Average Coef`))



# Merge all data frames based on variable names
combined_df <- full_join(lasso_df, enet_df, by = "variable") %>%
  mutate(across(where(is.numeric), ~ round(.x, 4))) %>%
  mutate(across(where(is.numeric), ~ ifelse(as.numeric(.) > large_threshold, "*", .)))

# Remove the intercept row
combined_df <- combined_df[combined_df$variable != "(Intercept)", ]

# Standardize interaction terms by sorting them alphabetically
combined_df <- combined_df %>%
  mutate(
    variable = sapply(variable, function(x) {
      terms <- unlist(strsplit(x, ":"))
      if (length(terms) > 1) {
        paste(sort(terms), collapse = ":")
      } else {
        x
      }
    })
  )

# Combine rows with the same standardized interaction term names
combined_df <- combined_df %>%
  group_by(variable) %>%
  summarize(across(everything(), ~ ifelse(is.numeric(.), sum(as.numeric(.), na.rm = TRUE), .))) %>%
  ungroup()

# Replace zeroes and any remaining NAs with an empty space for readability
combined_df <- combined_df %>%
  mutate(across(where(is.numeric), ~ ifelse(. == 0, "", .))) %>%
  replace(is.na(.), " ")

# Display the final combined table with grouped headers
combined_df %>%
  kable(row.names = F, 
        col.names = c("Variable", "Selection Times","Average Coef", "OR", "Selection Times", "Average Coef", "OR"),
        caption = "Summary of Average Coefficients and Odds Ratios for Potential Predictor Effects across Model Selection Methods") %>%
  add_header_above(c(" " = 1, "Lasso" = 3, "Elastic Net" = 3)) %>%
  kable_styling(full_width = F, position = "center", font_size = 9)

```


```{r}
# Initialize lists to store results for each imputation
roc_results_enet <- list()
calib_results_enet <- list()
auc_values_enet <- list()

roc_results_lasso <- list()
calib_results_lasso <- list()
auc_values_lasso <- list()

num_cuts <- 10  # Number of bins for calibration

for (i in 1:5) {
  dataset <- complete(imputed_data, action = i)
  
  # Split the dataset into training and test sets
  set.seed(46)
  train_index <- createDataPartition(dataset$abst, p = 0.7, list = FALSE)
  train_data <- dataset[train_index, ]
  test_data <- dataset[-train_index, ]
  
  # Prepare predictors and outcome for test set
  test_variables_dummy <- model.matrix(~ 0 + ., data = test_data[, predictor_names])
  test_variables_dummy <- test_variables_dummy[, -which(colnames(test_variables_dummy) == "Var0")]
  test_variables_dummy_full_interactions <- model.matrix(~ . ^ 2, data = as.data.frame(test_variables_dummy))
  test_variables_dummy_include <- test_variables_dummy_full_interactions[, train_predictors_dummy_include_names]
  test_outcome <- test_data$abst

  # Predict probabilities for Elastic Net
  predicted_prob_enet <- as.numeric(predict(aim2_results[[i]]$elastic_net,
                                            newx = as.matrix(test_variables_dummy_include),
                                            s = "lambda.min", type = "response"))
  
  # Predict probabilities for Lasso
  predicted_prob_lasso <- as.numeric(predict(aim2_results[[i]]$lasso,
                                             newx = as.matrix(test_variables_dummy_include),
                                             s = "lambda.min", type = "response"))

  # Compute ROC and AUC for Elastic Net
  roc_enet <- roc(test_outcome, predicted_prob_enet)
  roc_results_enet[[i]] <- data.frame(
    Specificity = rev(roc_enet$specificities),
    Sensitivity = rev(roc_enet$sensitivities)
  )
  auc_values_enet[[i]] <- auc(roc_enet)  # Store AUC separately
  
  # Compute ROC and AUC for Lasso
  roc_lasso <- roc(test_outcome, predicted_prob_lasso)
  roc_results_lasso[[i]] <- data.frame(
    Specificity = rev(roc_lasso$specificities),
    Sensitivity = rev(roc_lasso$sensitivities)
  )
  auc_values_lasso[[i]] <- auc(roc_lasso)  # Store AUC separately

  
  # Calibration for Elastic Net
  calib_data_enet <- data.frame(
    prob = predicted_prob_enet,
    bin = cut(predicted_prob_enet, breaks = num_cuts),
    class = as.numeric(test_outcome) - 1
  )
  calib_results_enet[[i]] <- calib_data_enet %>%
    group_by(bin) %>%
    summarise(
      observed = mean(class),
      predicted = mean(prob),
      se = sqrt(observed * (1 - observed) / n())
    )
  
  # Calibration for Lasso
  calib_data_lasso <- data.frame(
    prob = predicted_prob_lasso,
    bin = cut(predicted_prob_lasso, breaks = num_cuts),
    class = as.numeric(test_outcome) - 1
  )
  calib_results_lasso[[i]] <- calib_data_lasso %>%
    group_by(bin) %>%
    summarise(
      observed = mean(class),
      predicted = mean(prob),
      se = sqrt(observed * (1 - observed) / n())
    )
}


```

```{r}
# Extract numeric AUC values from the list of AUC objects
auc_numeric_enet <- sapply(auc_values_enet, function(x) as.numeric(x))
auc_numeric_lasso <- sapply(auc_values_lasso, function(x) as.numeric(x))

# Compute the mean AUC
mean_auc_enet <- mean(auc_numeric_enet)
mean_auc_lasso <- mean(auc_numeric_lasso)

```


```{r}
# Define a common set of specificity thresholds
common_specificities <- seq(0, 1, length.out = 100)

# Interpolate sensitivity for each ROC curve at the common specificities
interp_sensitivities_lasso <- sapply(roc_results_lasso, function(roc_data) {
  approx(x = roc_data$Specificity, y = roc_data$Sensitivity, xout = common_specificities)$y
})

# Compute the mean sensitivity across imputations
mean_sensitivity_lasso <- rowMeans(interp_sensitivities_lasso, na.rm = TRUE)

# Create a data frame for the averaged ROC curve
mean_roc_lasso <- data.frame(
  Specificity = common_specificities,
  Sensitivity = mean_sensitivity_lasso
)

# Plot the averaged ROC curve
ROC_lasso = ggplot(mean_roc_lasso, aes(x = 1 - Specificity, y = Sensitivity)) +
  geom_line(color = "black", size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "grey") +
  annotate("text", x = 0.8, y = 0.2, label = paste("Mean AUC =", round(mean_auc_lasso, 2)), size = 3, color = "Black") +
  labs(
    x = "1 - Specificity",
    y = "Sensitivity"
  ) +
  theme_minimal()

```

```{r}
# Define a common set of specificity thresholds
common_specificities <- seq(0, 1, length.out = 100)

# Interpolate sensitivity for each ROC curve at the common specificities
interp_sensitivities_enet <- sapply(roc_results_enet, function(roc_data) {
  approx(x = roc_data$Specificity, y = roc_data$Sensitivity, xout = common_specificities)$y
})

# Compute the mean sensitivity across imputations
mean_sensitivity_enet <- rowMeans(interp_sensitivities_enet, na.rm = TRUE)

# Create a data frame for the averaged ROC curve
mean_roc_enet <- data.frame(
  Specificity = common_specificities,
  Sensitivity = mean_sensitivity_enet
)

# Plot the averaged ROC curve
ROC_enet = ggplot(mean_roc_enet, aes(x = 1 - Specificity, y = Sensitivity)) +
  geom_line(color = "black", size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "grey") +
  annotate("text", x = 0.8, y = 0.2, label = paste("Mean AUC =", round(mean_auc_enet, 2)), size = 3, color = "Black") +
  labs(
    x = "1 - Specificity",
    y = "Sensitivity"
  ) +
  theme_minimal()

```


```{r}
# Combine Calibration Results Across Imputations
calib_combined_enet <- do.call(rbind, calib_results_enet) %>%
  group_by(bin) %>%
  summarise(
    observed = mean(observed),
    predicted = mean(predicted),
    se = sqrt(sum(se^2) / n())
  )

calib_combined_lasso <- do.call(rbind, calib_results_lasso) %>%
  group_by(bin) %>%
  summarise(
    observed = mean(observed),
    predicted = mean(predicted),
    se = sqrt(sum(se^2) / n())
  )

```

```{r}
num_cuts <- 10  # Number of bins for calibration

# Add Loess Fit for Flexible Calibration Line
loess_fit <- loess(observed ~ predicted, data = calib_combined_lasso, span = 0.75)
calib_combined_lasso$loess_pred <- predict(loess_fit, calib_combined_lasso$predicted)

# Plot Calibration Curve with Error Bars
calib_error_bar_lasso = ggplot(calib_combined_lasso) + 
  geom_abline(intercept = 0, slope = 1, color = "red") + 
  geom_errorbar(aes(x = predicted, ymin = observed - 1.96 * se, 
                    ymax = observed + 1.96 * se), 
                colour="black", width=.01)+
  geom_point(aes(x = predicted, y = observed)) +
  labs(x = "Expected Probability of Smoking Abstinence", 
       y = "Actual Smoking Abstinence") +
  #       title = "Calibration Plot for Elastic Net Model with Error Bars"
  theme_minimal()


# Plot Calibration Curve with Loess
calib_combined_lasso <- calib_combined_lasso %>%
  mutate(loess_ci_lower = loess_pred - 1.96 * sd(loess_pred),
         loess_ci_upper = loess_pred + 1.96 * sd(loess_pred))

calib_loess_lasso = ggplot(calib_combined_lasso, aes(x = predicted, y = observed)) +
  # Flexible calibration (Loess)
  geom_line(aes(y = loess_pred), color = "blue", linetype = "dashed") +  
  geom_ribbon(aes(ymin = loess_ci_lower, ymax = loess_ci_upper), alpha = 0.2, fill = "grey") +
  geom_abline(intercept = 0, slope = 1, color = "red") +  # Perfect calibration line
  scale_color_manual(values = c("Ideal" = "red", 
                                "Flexible calibration" = "blue")) +
  scale_linetype_manual(values = c("Ideal" = "solid", 
                                   "Flexible calibration" = "dashed")) +
  labs(x = "Predicted Probability of Smoking Abstinence", 
       y = "Actual Smoking Abstinence",
       color = "Legend", linetype = "Legend") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) 

```


```{r}
num_cuts <- 10  # Number of bins for calibration

# Add Loess Fit for Flexible Calibration Line
loess_fit <- loess(observed ~ predicted, data = calib_combined_enet, span = 0.75)
calib_combined_enet$loess_pred <- predict(loess_fit, calib_combined_enet$predicted)

# Plot Calibration Curve with Error Bars
calib_error_bar_enet = ggplot(calib_combined_enet) + 
  geom_abline(intercept = 0, slope = 1, color = "red") + 
  geom_errorbar(aes(x = predicted, ymin = observed - 1.96 * se, 
                    ymax = observed + 1.96 * se), 
                colour="black", width=.01)+
  geom_point(aes(x = predicted, y = observed)) +
  labs(x = "Expected Probability of Smoking Abstinence", 
       y = "Actual Smoking Abstinence") +
  #       title = "Calibration Plot for Elastic Net Model with Error Bars"
  theme_minimal()


# Plot Calibration Curve with Loess
calib_combined_enet <- calib_combined_enet %>%
  mutate(loess_ci_lower = loess_pred - 1.96 * sd(loess_pred),
         loess_ci_upper = loess_pred + 1.96 * sd(loess_pred))

calib_loess_enet = ggplot(calib_combined_enet, aes(x = predicted, y = observed)) +
  # Flexible calibration (Loess)
  geom_line(aes(y = loess_pred), color = "blue", linetype = "dashed") +  
  geom_ribbon(aes(ymin = loess_ci_lower, ymax = loess_ci_upper), alpha = 0.2, fill = "grey") +
  geom_abline(intercept = 0, slope = 1, color = "red") +  # Perfect calibration line
  scale_color_manual(values = c("Ideal" = "red", 
                                "Flexible calibration" = "blue")) +
  scale_linetype_manual(values = c("Ideal" = "solid", 
                                   "Flexible calibration" = "dashed")) +
  labs(x = "Predicted Probability of Smoking Abstinence", 
       y = "Actual Smoking Abstinence",
       color = "Legend", linetype = "Legend") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5)) 

```


```{r, fig.width = 10, fig.height = 6, out.width="90%", fig.align='center'}

plots_enet = arrangeGrob(
  calib_error_bar_enet, calib_loess_enet,  ROC_enet,
  ncol = 3,
  top = textGrob("Elastic Net Regression",
                 gp = gpar(fontface = "bold", fontsize = 14)
))

plots_lasso = arrangeGrob(
  calib_error_bar_lasso, calib_loess_lasso, ROC_lasso,
  ncol = 3,
  top = textGrob("Lasso Regression", 
                 gp = gpar(fontface = "bold", fontsize = 14)
))

# Bold the main title
main_title <- textGrob(
  "Figure 2: Calibration Plots with Error Bars and LOESS and ROC Curves (Predictor Effects)",
  gp = gpar(fontsize = 16) 
)

# Arrange everything with the bold title
grid.arrange(
  plots_lasso,
  plots_enet,
  nrow = 2,
  top = main_title
)
```

In examining the coefficients and odds ratios for baseline characteristics as predictors, the Nicotine Metabolism Ratio (`NMR`) and FTCD score (`ftcd_score`) stood out as significant predictors across variable selection methods. `NMR` shows relatively high OR $>1$ in all models indicating its potential strong positive association with smoking cessation outcomes. On the opposite, `ftcd_score` consistently shows OR $<1$ in all three methods, which may be explained by the rationale that the higher the baseline cigarette dependence, the lower the probability to quit smoking successfully at the end of the trial.

Other demographic variables, such as race, showed generally low OR $<<1$ (i.e., `raceOther` and `raceBlack`), suggesting possible barriers to cessation in certain groups. Additionally, the education indicators, `edu_merged2` and `edu_merged3`, present odds ratios that differ in directionality, with `edu_merged2` consistently showing OR $< 1$, while `edu_merged3` displays OR $> 1$ across methods. This may suggest that different levels of educational attainment may have opposing effect directionalities on the success likelihood of smoking cessation, potentially reflecting the influence of socioeconomic factors associated with varying educational backgrounds. All these findings highlight the complex interplay between socioeconomic, biological, and demographic factors in influencing smoking cessation success.

Noticeably, pharmacotherapy, as one of the controls, consistently demonstrated a stronger influence on abstinence outcomes than behaviorial treatment, which aligns with previous findings in the original study. `Var` exhibits significantly higher odds ratios across models, particularly in the stepwise and best subset methods, underscoring its greater predictive effect on smoking cessation. In contrast, `BA` shows odds ratios close to 1, suggesting its modest (to negative) impact on the outcome.



In terms of model performance, shown in **Table 7**, Elastic Net still exhibited the best calibration metrics overall, with the lowest Brier score (0.1518) and calibration error (0.1118), indicating it achieves the most accurate probability estimates compared to the other two selection methods. However, both stepwise and best subset methods demonstrated slightly better AUC values (0.6630 and 0.6605, respectively), suggesting they may provide slightly stronger discrimination between abstinence outcomes. 

Interestingly, the highest sensitivity was identical across all three methods at 0.8750, in contrast to the findings in *Objective 1*, where sensitivity varied more between methods. This convergence in sensitivity may suggest that when the model complexity is reduced, meaning focusing on baseline predictors without interaction terms, the sensitivity of each method aligns more closely, resulting in more consistent classification performance across methods. Overall, these results suggest that Elastic Net offers the most reliable calibration, while bidirectional stepwise and best subset maintain slightly better discrimination, balancing predictive needs depending on whether calibration or discrimination is prioritized.




# \textcolor{orange}{CONCLUSION}

This study reexamined factors influencing smoking cessation among adults with MDD, focusing on identifying potential moderators of behavioral activation treatment effects on smoking abstinence and evaluating baseline predictors of cessation outcomes. For both analysis objectives, we utilized stepwise selection, Elastic Net, and best subset methods to select potentially meaningful factors. Key variables like the FTCD score and readiness to quit emerged as plausible moderators, suggesting that the effectiveness of BASC may depend on individual differences in nicotine dependence and motivation to quit. Despite method-specific variations, these results underscore the complexity of identifying consistent moderators, though with a small sample size and numerous interaction terms.

In the predictor-focused analysis, several baseline characteristics stood out for their predictive power in smoking cessation outcomes, including nicotine metabolism ratio (NMR), FTCD score, education, and race. Notably, NMR consistently showed a strong association with cessation, potentially indicating a biological influence on treatment success. Additionally, educational levels presented opposing odds ratios, with some levels being positively associated with cessation while others were negatively associated, possibly reflecting varying socio-economic backgrounds. This predictor analysis highlights the nuanced influences of socioeconomic, biological, and demographic factors on smoking cessation among individuals with MDD.

Regarding prediction performance of the three model selection methods employed, elastic net demonstrated the best calibration performance across both objectives, achieving the lowest Brier score and calibration error, while other methods provided slightly better discrimination as evidenced by higher AUCs. These findings suggest that elastic net would be a robust choice when calibration is prioritized, but stepwise and best subset approaches may offer value in contexts where discrimination is more critical. 

Overall, the results of our project provided some insights that could guide future study design when developing smoking cessation interventions for adults with MDD. By highlighting specific baseline moderators and predictors of the cessation outcome, this work may support a more targeted approach to intervention planning in considering baseline smoking behaviors and socio-demographics, potentially improving treatment efficacy for this population.

# \textcolor{orange}{LIMITATIONS}

A primary limitation of this analysis is the small sample size, with only 300 observations split between training and test sets. This limited data size, combined with the inclusion of multiple interaction terms, exacerbates the "small n, big p" problem, where the number of predictors may outstrip the available data points, potentially leading to overfitting and instability in model estimates. Given the small dataset, further research with larger samples and significance testing of individual predictors would enhance the reliability and generalizability of these conclusions.

Additionally, the predictor selection process was based solely on criteria like AIC for birectional stepwise selection, cross-validation for Elastic Net, or Mallow's $Cp$ for best subset, focusing on optimizing overall model performance metrics, such as calibration and discrimination, rather than testing the individual significance of each variable. As a result, no formal $p$-values or statistical tests were assigned to assess the significance of individual variables. This approach limits our ability to make definitive claims about the significance of individual variables as verified moderators or predictors and instead relies on their contribution to the model’s overall predictive performance.

# Consent, Data, and Code Availability

Primary data were provided by Dr. George Papandonatos from the Department of Biostatistics at Brown University. The original data cannot be shared directly for privacy. Replication scripts are available at <https://github.com/YanweiTong-Iris/PHP2550-ProjectPortfolio/tree/main/Project%202>.

# Reference
<div id="refs"></div>


\pagebreak

# Code Appendix

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
